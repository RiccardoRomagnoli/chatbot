Testing: LogisticRegression
Confusion matrix, without normalization
[[10  0  0  0  0]
 [ 1  7  0  0  0]
 [ 0  0  4  2  0]
 [ 1  0  0  8  1]
 [ 0  0  0  0  8]]
Normalized confusion matrix
[[1.         0.         0.         0.         0.        ]
 [0.125      0.875      0.         0.         0.        ]
 [0.         0.         0.66666667 0.33333333 0.        ]
 [0.1        0.         0.         0.8        0.1       ]
 [0.         0.         0.         0.         1.        ]]

The accuracy of LogisticRegression is 0.8809523809523809
              precision    recall  f1-score   support

           0       0.83      1.00      0.91        10
           1       1.00      0.88      0.93         8
           2       1.00      0.67      0.80         6
           5       0.80      0.80      0.80        10
           6       0.89      1.00      0.94         8

    accuracy                           0.88        42
   macro avg       0.90      0.87      0.88        42
weighted avg       0.89      0.88      0.88        42

Testing: LinearSVC
Confusion matrix, without normalization
[[10  0  0  0  0]
 [ 1  7  0  0  0]
 [ 0  0  4  2  0]
 [ 1  0  0  8  1]
 [ 0  0  0  0  8]]
Normalized confusion matrix
[[1.         0.         0.         0.         0.        ]
 [0.125      0.875      0.         0.         0.        ]
 [0.         0.         0.66666667 0.33333333 0.        ]
 [0.1        0.         0.         0.8        0.1       ]
 [0.         0.         0.         0.         1.        ]]

The accuracy of LinearSVC is 0.8809523809523809
              precision    recall  f1-score   support

           0       0.83      1.00      0.91        10
           1       1.00      0.88      0.93         8
           2       1.00      0.67      0.80         6
           5       0.80      0.80      0.80        10
           6       0.89      1.00      0.94         8

    accuracy                           0.88        42
   macro avg       0.90      0.87      0.88        42
weighted avg       0.89      0.88      0.88        42

Testing: RandomForestClassifier
Confusion matrix, without normalization
[[1 0 0 9 0]
 [1 7 0 0 0]
 [0 0 4 1 1]
 [0 0 0 7 3]
 [0 0 0 0 8]]
Normalized confusion matrix
[[0.1        0.         0.         0.9        0.        ]
 [0.125      0.875      0.         0.         0.        ]
 [0.         0.         0.66666667 0.16666667 0.16666667]
 [0.         0.         0.         0.7        0.3       ]
 [0.         0.         0.         0.         1.        ]]

The accuracy of RandomForestClassifier is 0.6428571428571429
              precision    recall  f1-score   support

           0       0.50      0.10      0.17        10
           1       1.00      0.88      0.93         8
           2       1.00      0.67      0.80         6
           5       0.41      0.70      0.52        10
           6       0.67      1.00      0.80         8

    accuracy                           0.64        42
   macro avg       0.72      0.67      0.64        42
weighted avg       0.68      0.64      0.61        42

Testing: MultinomialNB
Confusion matrix, without normalization
[[1 0 0 0 9]
 [1 7 0 0 0]
 [0 1 4 0 1]
 [0 0 0 6 4]
 [0 0 0 0 8]]
Normalized confusion matrix
[[0.1        0.         0.         0.         0.9       ]
 [0.125      0.875      0.         0.         0.        ]
 [0.         0.16666667 0.66666667 0.         0.16666667]
 [0.         0.         0.         0.6        0.4       ]
 [0.         0.         0.         0.         1.        ]]

The accuracy of MultinomialNB is 0.6190476190476191
              precision    recall  f1-score   support

           0       0.50      0.10      0.17        10
           1       0.88      0.88      0.88         8
           2       1.00      0.67      0.80         6
           5       1.00      0.60      0.75        10
           6       0.36      1.00      0.53         8

    accuracy                           0.62        42
   macro avg       0.75      0.65      0.62        42
weighted avg       0.74      0.62      0.60        42
